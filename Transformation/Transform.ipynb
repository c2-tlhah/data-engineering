{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\c2tlha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\sql\\context.py:113: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               order_id                       customer_id  \\\n",
      "0      e481f51cbdc54678b7cc49136f2d6af7  9ef432eb6251297304e76186b10a928d   \n",
      "1      53cdb2fc8bc7dce0b6741e2150273451  b0830fb4747a6c6d20dea0b8c802d7ef   \n",
      "2      47770eb9100c2d0c44946d9cf07ec65d  41ce2a54c0b03bf3443c3d931a367089   \n",
      "3      949d5b44dbf5de918fe9c16f97b45f8a  f88197465ea7920adcdbec7375364d82   \n",
      "4      ad21c59c0840e6cb83a9ceb5573f8159  8ab97904e6daea8866dbdbc4fb7aad2c   \n",
      "...                                 ...                               ...   \n",
      "99436  9c5dedf39a927c1b2549525ed64a053c  39bd1228ee8140590ac3aca26f2dfe00   \n",
      "99437  63943bddc261676b46f01ca7ac2f7bd8  1fca14ff2861355f6e5f14306ff977a7   \n",
      "99438  83c1379a015df1e13d02aae0204711ab  1aa71eb042121263aafbe80c1b562c9c   \n",
      "99439  11c177c8e97725db2631073c19f07b62  b331b74b18dc79bcdf6532d51e1637c1   \n",
      "99440  66dea50a8b16d9b4dee7af250b4be1a5  edb027a75a1449115f6b43211ae02a24   \n",
      "\n",
      "      order_status order_purchase_timestamp    order_approved_at  \\\n",
      "0        delivered      2017-10-02 10:56:33  2017-10-02 11:07:15   \n",
      "1        delivered      2018-07-24 20:41:37  2018-07-26 03:24:27   \n",
      "2        delivered      2018-08-08 08:38:49  2018-08-08 08:55:23   \n",
      "3        delivered      2017-11-18 19:28:06  2017-11-18 19:45:59   \n",
      "4        delivered      2018-02-13 21:18:39  2018-02-13 22:20:29   \n",
      "...            ...                      ...                  ...   \n",
      "99436    delivered      2017-03-09 09:54:05  2017-03-09 09:54:05   \n",
      "99437    delivered      2018-02-06 12:58:58  2018-02-06 13:10:37   \n",
      "99438    delivered      2017-08-27 14:46:43  2017-08-27 15:04:16   \n",
      "99439    delivered      2018-01-08 21:28:27  2018-01-08 21:36:21   \n",
      "99440    delivered      2018-03-08 20:57:30  2018-03-09 11:20:28   \n",
      "\n",
      "      order_delivered_carrier_date order_delivered_customer_date  \\\n",
      "0              2017-10-04 19:55:00           2017-10-10 21:25:13   \n",
      "1              2018-07-26 14:31:00           2018-08-07 15:27:45   \n",
      "2              2018-08-08 13:50:00           2018-08-17 18:06:29   \n",
      "3              2017-11-22 13:39:59           2017-12-02 00:28:42   \n",
      "4              2018-02-14 19:46:34           2018-02-16 18:17:02   \n",
      "...                            ...                           ...   \n",
      "99436          2017-03-10 11:18:03           2017-03-17 15:08:01   \n",
      "99437          2018-02-07 23:22:42           2018-02-28 17:37:56   \n",
      "99438          2017-08-28 20:52:26           2017-09-21 11:24:17   \n",
      "99439          2018-01-12 15:35:03           2018-01-25 23:32:54   \n",
      "99440          2018-03-09 22:11:59           2018-03-16 13:08:30   \n",
      "\n",
      "      order_estimated_delivery_date  \n",
      "0               2017-10-18 00:00:00  \n",
      "1               2018-08-13 00:00:00  \n",
      "2               2018-09-04 00:00:00  \n",
      "3               2017-12-15 00:00:00  \n",
      "4               2018-02-26 00:00:00  \n",
      "...                             ...  \n",
      "99436           2017-03-28 00:00:00  \n",
      "99437           2018-03-02 00:00:00  \n",
      "99438           2017-09-27 00:00:00  \n",
      "99439           2018-02-15 00:00:00  \n",
      "99440           2018-04-03 00:00:00  \n",
      "\n",
      "[99441 rows x 8 columns]\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'distutils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(df)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# If you need to convert the pandas DataFrame to a Spark DataFrame\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m spark_df \u001b[38;5;241m=\u001b[39m \u001b[43msqlContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Show the Spark DataFrame schema\u001b[39;00m\n\u001b[0;32m     43\u001b[0m spark_df\u001b[38;5;241m.\u001b[39mprintSchema()\n",
      "File \u001b[1;32mc:\\Users\\c2tlha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\sql\\context.py:486\u001b[0m, in \u001b[0;36mSQLContext.createDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreateDataFrame\u001b[39m(  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    382\u001b[0m     data: Union[RDD[Any], Iterable[Any], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPandasDataFrameLike\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    385\u001b[0m     verifySchema: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    386\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m    387\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;124;03m    Creates a :class:`DataFrame` from an :class:`RDD`, a list or a :class:`pandas.DataFrame`.\u001b[39;00m\n\u001b[0;32m    389\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;124;03m    Py4JJavaError: ...\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 486\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\c2tlha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\sql\\session.py:1440\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m   1436\u001b[0m     data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data, columns\u001b[38;5;241m=\u001b[39mcolumn_names)\n\u001b[0;32m   1438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[1;32m-> 1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSparkSession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1441\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\n\u001b[0;32m   1442\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_dataframe(\n\u001b[0;32m   1444\u001b[0m     data, schema, samplingRatio, verifySchema  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1445\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\c2tlha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:330\u001b[0m, in \u001b[0;36mSparkConversionMixin.createDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, SparkSession)\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m require_minimum_pandas_version\n\u001b[1;32m--> 330\u001b[0m \u001b[43mrequire_minimum_pandas_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    332\u001b[0m timezone \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jconf\u001b[38;5;241m.\u001b[39msessionLocalTimeZone()\n\u001b[0;32m    334\u001b[0m \u001b[38;5;66;03m# If no schema supplied by user then get the names of columns only\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\c2tlha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\sql\\pandas\\utils.py:24\u001b[0m, in \u001b[0;36mrequire_minimum_pandas_version\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# TODO(HyukjinKwon): Relocate and deduplicate the version specification.\u001b[39;00m\n\u001b[0;32m     22\u001b[0m minimum_pandas_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.0.5\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdistutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LooseVersion\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'distutils'"
     ]
    }
   ],
   "source": [
    "# Corrected path using raw string\n",
    "path = r'C:\\Users\\c2tlha\\Desktop\\DATA ENGINEERING PROJECT\\Dataset\\terencicp\\olist.sqlite'\n",
    "\n",
    "# Import necessary libraries\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "from pyspark.sql.functions import from_unixtime\n",
    "\n",
    "spark = SparkSession.builder.appName(\"olist\").getOrCreate()\n",
    "\n",
    "sqlContext = SQLContext(spark)\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect(path)# Corrected path using raw string\n",
    "path = r'C:\\Users\\c2tlha\\Desktop\\DATA ENGINEERING PROJECT\\Dataset\\terencicp\\olist.sqlite'\n",
    "\n",
    "# Import necessary libraries\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "from pyspark.sql.functions import from_unixtime\n",
    "\n",
    "# Verify if distutils is installed\n",
    "try:\n",
    "    from distutils.version import LooseVersion\n",
    "    print(\"distutils is installed.\")\n",
    "except ImportError:\n",
    "    print(\"distutils is not installed. Please install it.\")\n",
    "    # Optionally, exit the script or handle the error\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"olist\").getOrCreate()\n",
    "\n",
    "# Initialize SQLContext\n",
    "sqlContext = SQLContext(spark)\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect(path)\n",
    "\n",
    "# Execute a SQL query to retrieve data from the 'orders' table\n",
    "query = \"SELECT * FROM orders\"\n",
    "\n",
    "# Load the data into a pandas DataFrame\n",
    "df = pd.read_sql_query(query, conn)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "\n",
    "# If you need to convert the pandas DataFrame to a Spark DataFrame\n",
    "spark_df = sqlContext.createDataFrame(df)\n",
    "\n",
    "# Show the Spark DataFrame schema\n",
    "spark_df.printSchema()\n",
    "\n",
    "# Show the Spark DataFrame\n",
    "spark_df.show()\n",
    "\n",
    "\n",
    "# Execute a SQL query to retrieve data from the 'orders' table\n",
    "query = \"SELECT * FROM orders\"\n",
    "\n",
    "# Load the data into a pandas DataFrame\n",
    "df = pd.read_sql_query(query, conn)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "\n",
    "# If you need to convert the pandas DataFrame to a Spark DataFrame\n",
    "spark_df = sqlContext.createDataFrame(df)\n",
    "\n",
    "# Show the Spark DataFrame schema\n",
    "spark_df.printSchema()\n",
    "\n",
    "# Show the Spark DataFrame\n",
    "spark_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
